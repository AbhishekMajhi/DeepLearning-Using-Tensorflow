I am trying to build a voice assistant for my self. Where I will implement all of my thoughts that I have for this voice assistant.
I will make it more advance as much as possible with current deep learning algorithems and models discovered.
So here as a test part I build this model using LSTM and Glove Word embedding, which will help my assistant to understant the similarity between commands said by user.
Exa: If I want to know the weather of the day. Then I can say:
  What will be the weather today.
 or 
  I planned to go somewhere so what will be the day.
 As you can see here both the commands refers to same thing. This will be the model job to find.
 
But the problem here that as we have very little train dataset of 140 sentences and we will output/predict a label for each command category passed to our model(Exa: "whats the time"-> 0
,"who are you"-> 1), and we have 15 labels or we can say 15 different commands. 
So in future when we have more actions for our assistant to perform so we will have more commands or more labels or more classes to predict. 
At this point our model will have overfitting or high variance issue.
We can deal with this issue with having a bigger dataset or with a better model architecture. But in this case creating a big dataset is not a option to go for. 
So we can go for transfer learning or try a pretrained saimese MALSTM network then use transfer learning on it.
So experiment is going on my side and I will upload that when it finished.
